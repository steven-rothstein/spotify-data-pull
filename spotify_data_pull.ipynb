{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871dd557",
   "metadata": {},
   "source": [
    "# A Brief Analysis of A Spotify User's \"Liked Songs\" Playlist\n",
    "\n",
    "This notebook is meant to give an example of how Spotify users can download and inspect their data.\n",
    "\n",
    "In this instance, the configured user from the YAML file is logged in via Selenium to authenticate the OAuth 2.0 interaction. Then, the user's liked songs are donwloaded, and a variety of analyses ensue. Other API calls are made to inspect various API offerings along the way as well.\n",
    "\n",
    "Finally, some cleaned and aggregated data are saved into flat files for use in other visualization platforms. Please see the README for a full dictionary.\n",
    "\n",
    "In this notebook, outputs' artist and track names can be hashed for privacy purposes. These lines can easily be commented in/out, and are notated within the code itself. Where displayed, IDs are left as-is if readers really want to go to the proper lengths to explore the example user's liked songs ðŸ™‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edef27c",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Local Library Setup](#locallibrarysetup)\n",
    "    - [Package Imports](#packageimports)\n",
    "    - [Display Preferences](#displaypreferences)\n",
    "    - [Functions](#functions)\n",
    "        - [General Data Cleansing](#generaldatacleansing)\n",
    "        - [Spotify-Specific Functions](#spotifyspecificfunctions)\n",
    "- [Script Start](#scriptstart)\n",
    "    - [Authentication](#authentication)\n",
    "        - [Selenium-Based OAuth 2.0 Authentication](#selenium)\n",
    "        - [Bearer Token Retrieval](#bearertokenretrieval)\n",
    "    - [Obtain \"Liked\" Track Details](#obtainlikedtrackdetails)\n",
    "    - [Timeline of When Tracked Are Liked](#timeline)\n",
    "    - [Additional API Exploration](#additionalapiexploration)\n",
    "        - [Long Term Top Tracks](#longtermtoptracks)\n",
    "        - [Followed Artists](#followedartists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0352a67a",
   "metadata": {},
   "source": [
    "## Local Library Set Up <a name=\"locallibrarysetup\"></a>\n",
    "\n",
    "This section imports packages, sets display preferences, and defines functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c4f5e5",
   "metadata": {},
   "source": [
    "### Package Imports <a name=\"packageimports\"></a>\n",
    "\n",
    "`json`, `requests`, `jmespath`, and `base64` handle making API requests and analyzing the responses.\n",
    "\n",
    "`yaml` is used strictly for configuration files. See the README for usage instructions.\n",
    "\n",
    "The `ipywdigets` package is used to dislpay a live progress bar for heavier API requests.\n",
    "\n",
    "`numpy`, `matplotlib.pyplot`, and `pandas` are crucial to quickly analyize, transform, and visualize data.\n",
    "\n",
    "`selenium` and `webdriver_manager.chrome` are needed to input the Spotify user credentials into the app to authenticate the interaction via Oauth 2.0.\n",
    "\n",
    "Lastly, the `time` package allows for a brief seconds-long pause to ensure web interactions complete successfully before processing continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import jmespath\n",
    "import base64\n",
    "import yaml\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad92f0",
   "metadata": {},
   "source": [
    "### Display Preferences <a name=\"displaypreferences\"></a>\n",
    "\n",
    "The number of columns was not restricted to inspect all of the data's fields in this application. Additionally, a large amount of rows to display helped achieve this same goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56776443",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7612f9a",
   "metadata": {},
   "source": [
    "### Functions <a name=\"functions\"></a>\n",
    "\n",
    "This section defines all of the custom functions needed throughout the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fab2bad",
   "metadata": {},
   "source": [
    "#### General Data Cleansing <a name=\"generaldatacleansing\"></a>\n",
    "\n",
    "These functions are not unique to Spotify purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "# A field of JSON from a row of data is converted to its own dataframe\n",
    "# In addtion, the unique id columns from its source are appended on as the first columns.\n",
    "# Args:\n",
    "# pd_series is a row of data from a pandas DataFrame.\n",
    "# id_names is either a string of the id column's name, or a list of strings of the id columns' names.\n",
    "# json_col_to_parse is the name of the JSON column to convert to a DataFrame.\n",
    "def assign_id_and_parse(pd_series, id_names, json_col_to_parse):\n",
    "    # Convert to a list if not one already\n",
    "    id_names = id_names if isinstance(id_names, list) else [id_names]\n",
    "    \n",
    "    # Convert the nested lists of dictionaries to a DataFrame\n",
    "    retVal = pd.DataFrame(pd_series[json_col_to_parse])\n",
    "    \n",
    "    # Insert the current ID as the first column, going backwards through the list.\n",
    "    for curr_id_name in id_names[::-1]:\n",
    "        retVal.insert(0, curr_id_name, pd_series[curr_id_name])\n",
    "    \n",
    "    return retVal\n",
    "\n",
    "# This function takes a DataFrame and parses a column, for which each row is JSON.\n",
    "# Each JSON value is converted to its own DataFrame.\n",
    "# In addtion, for each new DataFrame, the unique id columns from its source are appended on as the first columns.\n",
    "# Finally, the resultant DataFrames are unioned together and returned.\n",
    "# Args:\n",
    "# df is the DataFrame to parse\n",
    "# id_col_names is either a string of the id column's name, or a list of strings of the id columns' names.\n",
    "# json_col_name is the name of the JSON column to convert to a DataFrame.\n",
    "def convert_json_col_to_dataframe_with_key(df, id_col_names, json_col_name):\n",
    "    retVal_list = list()\n",
    "    \n",
    "    # Go through all rows, parse the JSON, assign the unique ID(s). Union the results.\n",
    "    for i, df_row in df.iterrows():\n",
    "        retVal_list.append(assign_id_and_parse(df_row, id_col_names, json_col_name))\n",
    "    \n",
    "    return pd.concat(retVal_list).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70aadb6",
   "metadata": {},
   "source": [
    "#### Spotify-Specifc Functions <a name=\"spotifyspecificfunctions\"></a>\n",
    "\n",
    "These fuctions are specific to interacting with the Spotify API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7340941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function to write a pandas DataFrame into the data_out folder of the project setup.\n",
    "# Args:\n",
    "# pd_df is the pandas DataFrame\n",
    "# filename is the name for the new file to write\n",
    "def spotify_write_df_to_data_out_csv(pd_df, filename):\n",
    "    pd_df.to_csv(f'data_out/{filename}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de30c05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function to call the Spotify API\n",
    "# Args:\n",
    "# access_token: the token retrieved through Oauth 2.0\n",
    "# endpoint: the Spotify endpoint to hit\n",
    "# content_type: a string of the value to pass to the API Content-Type header\n",
    "# query: a dictionary of key value pairs to send via the API. Defaulted to an empty dictionary if not needed.\n",
    "# max_parse_level: passed to pd.normalize and controls how JSON is flattened. The default, 0, ensures max flattening.\n",
    "# base_obj: a string to pass if the returned JSON is wrapped in a tag. Used to filter out the tag for parsing efficiency.\n",
    "def spotify_get_all_results(access_token \\\n",
    "                            , endpoint \\\n",
    "                            , content_type \\\n",
    "                            , query = {} \\\n",
    "                            , max_parse_level = 0 \\\n",
    "                            , base_obj = None):\n",
    "    # Header setup\n",
    "    api_call_headers = {\n",
    "        'Authorization': 'Bearer ' + access_token,\n",
    "        'Content-Type': content_type\n",
    "    }\n",
    "\n",
    "    # Variable setup for loop to get all results\n",
    "    next_api_url = endpoint\n",
    "\n",
    "    first_call = True\n",
    "    retVal_list = list()\n",
    "\n",
    "    # Loop through the API-provided next endpoints until no more exist. Union the results.\n",
    "    while next_api_url is not None:        \n",
    "        # HTTP GET\n",
    "        # raise_for_status() will stop execution on this fatal error.\n",
    "        api_request = requests.get(next_api_url, headers = api_call_headers, params = query if first_call else {})\n",
    "        api_request.raise_for_status()\n",
    "\n",
    "        # Get the repsonse in JSON\n",
    "        api_request_json = api_request.json()\n",
    "        \n",
    "        # Filter out the base_obj if it exists\n",
    "        if base_obj is not None:\n",
    "            # Too simple for JMESPath...\n",
    "            # TODO convert to JMESPath if more complex use cases arise\n",
    "            api_request_json = api_request_json[base_obj]\n",
    "\n",
    "        # If the first call, determine how many pages of data the API will have to retrieve.\n",
    "        # Use this calculation to create a progress bar to display.\n",
    "        if first_call:\n",
    "            num_pages = int(np.ceil(api_request_json['total'] / api_request_json['limit']))\n",
    "\n",
    "            desc_style = {'description_width': 'initial'}\n",
    "            api_request_loading_bar = widgets.IntProgress(value = 0 \\\n",
    "                                                            , min = 0 \\\n",
    "                                                            , max = num_pages \\\n",
    "                                                            , description = 'Loading:' \\\n",
    "                                                            , bar_style = 'success' \\\n",
    "                                                            , orientation='horizontal' \\\n",
    "                                                            , style = desc_style\\\n",
    "                                                           )\n",
    "\n",
    "            first_call = False\n",
    "\n",
    "            display(api_request_loading_bar)\n",
    "\n",
    "        # Get the next endpoint to call, and convert the current JSON response to a DataFrame.\n",
    "        next_api_url = api_request_json['next']\n",
    "\n",
    "        retVal_list.append(pd.json_normalize(api_request_json['items'], max_level = max_parse_level))\n",
    "\n",
    "        # Update the progress bar\n",
    "        api_request_loading_bar.value += 1\n",
    "\n",
    "        if api_request_loading_bar.value < num_pages:\n",
    "            api_request_loading_bar.description = f'Loading Page: {api_request_loading_bar.value + 1} of {num_pages}'\n",
    "\n",
    "    # When processing is complete, stop showing the progress bar\n",
    "    api_request_loading_bar.layout.display = 'none'\n",
    "    \n",
    "    # Return the unioned result\n",
    "    return pd.concat(retVal_list).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7528a299",
   "metadata": {},
   "source": [
    "## Script Start <a name=\"scriptstart\"></a>\n",
    "\n",
    "The fun begins. Let's seem some data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6127c",
   "metadata": {},
   "source": [
    "### Authentication <a name=\"authentication\"></a>\n",
    "\n",
    "This section encapsulates obtaining the Spotify access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6def28f",
   "metadata": {},
   "source": [
    "#### Selenium-Based OAuth 2.0 Authentication <a name=\"selenium\"></a>\n",
    "\n",
    "This section uses Selenium to authenticate the application via OAuth 2.0. Rather than create a dedicated web app, this Selenium approach was taken to spend more time with the data than frontend development would have allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the web driver\n",
    "driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()))\n",
    "\n",
    "spotify_accounts_endpoint = 'https://accounts.spotify.com/'\n",
    "spotify_api_endpoint = 'https://api.spotify.com/v1/'\n",
    "\n",
    "# Get the app and user credentials from the YAML file\n",
    "# Note: this application is meant for local use only.\n",
    "# Never publish or give out your credentials, or leave them unencrypted in an untrusted location.\n",
    "with open('config/config.yml', 'r') as file:\n",
    "    config_contents = yaml.safe_load(file)\n",
    "    \n",
    "config_contents_creds = config_contents['creds']\n",
    "\n",
    "client_id = config_contents_creds['client_id']\n",
    "client_secret = config_contents_creds['client_secret']\n",
    "scopes = 'user-read-private user-read-email playlist-read-private user-follow-read user-top-read user-read-recently-played user-library-read'\n",
    "\n",
    "redirect_uri = config_contents['redirect_uri']\n",
    "\n",
    "oath_token_url = f'{spotify_accounts_endpoint}authorize?client_id={client_id}&response_type=code&redirect_uri={redirect_uri}&scope={scopes}'\n",
    "\n",
    "# Load the page and enter the username and password\n",
    "driver.get(oath_token_url)\n",
    "\n",
    "username_input = driver.find_element('id', 'login-username')\n",
    "username_input.send_keys(config_contents_creds['username'])\n",
    "\n",
    "password_input = driver.find_element('id', 'login-password')\n",
    "password_input.send_keys(config_contents_creds['password'])\n",
    "\n",
    "login_button = driver.find_element('id', 'login-button')\n",
    "login_button.click()\n",
    "\n",
    "# If needed, click the proper \"Accept\" button to proceed to the next page\n",
    "if (driver.current_url.startswith(f'{spotify_accounts_endpoint}en/authorize?')):\n",
    "    agree_button = driver.find_element('xpath', '//button[@data-testid=\"auth-accept\"]')\n",
    "    agree_button.click()\n",
    "\n",
    "# Sleep to ensure the page loads\n",
    "time.sleep(2)\n",
    "    \n",
    "# Finally, obtain the oauth initial token\n",
    "oauth_initial_token = driver.current_url.replace(f'{redirect_uri}/?code=', '')\n",
    "\n",
    "# Quit out of selenium-based items\n",
    "driver.close()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a667c3ac",
   "metadata": {},
   "source": [
    "#### Bearer Token Retrieval <a name=\"bearertokenretrieval\"></a>\n",
    "\n",
    "In this section, we finally retrieve our Spotify access token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007ab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for the API call to retrieve an access token\n",
    "base64_encoding = 'ascii'\n",
    "content_type_dictionary = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "\n",
    "# Headers with Base64 auth encoding\n",
    "get_bearer_token_headers = {\n",
    "  'Authorization': 'Basic ' + \\\n",
    "                    base64.b64encode(f'{client_id}:{client_secret}'.encode(base64_encoding)).decode(base64_encoding)\n",
    "} | content_type_dictionary\n",
    "\n",
    "# Payload\n",
    "get_bearer_token_payload = {\n",
    "    'grant_type': 'authorization_code',\n",
    "    'code': oauth_initial_token,\n",
    "    'redirect_uri': redirect_uri\n",
    "}\n",
    "\n",
    "# HTTP POST\n",
    "get_bearer_token_response = requests.post(f'{spotify_accounts_endpoint}api/token' \\\n",
    "                                          , headers = get_bearer_token_headers \\\n",
    "                                          , data = get_bearer_token_payload)\n",
    "\n",
    "# Crash on error (no automated data pipelines to disrupt here...)\n",
    "get_bearer_token_response.raise_for_status()\n",
    "\n",
    "# Read the resulting JSON and retrieve your access token!\n",
    "get_bearer_token_response_json = get_bearer_token_response.json()\n",
    "access_token = get_bearer_token_response_json['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e35362",
   "metadata": {},
   "source": [
    "### Obtain \"Liked\" Track Details <a name=\"obtainlikedtrackdetails\"></a>\n",
    "\n",
    "First, we hit the `/me/tracks` endpoint and clean the headers, removing \"*track.*\" from the field names and renaming the \"*id*\" field to \"*track_id*\" for clarity. Next, we convert the \"*added_at*\" field to a proper datetime from a String.\n",
    "\n",
    "Finally, we look at the first few rows of data and see what the shape of the data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48263ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_str = 'track'\n",
    "track_id_str = f'{track_str}_id'\n",
    "\n",
    "added_at_str = 'added_at'\n",
    "name_str = 'name'\n",
    "\n",
    "# API call happens here\n",
    "my_tracks = spotify_get_all_results(access_token \\\n",
    "                                    , f'{spotify_api_endpoint}me/tracks' \\\n",
    "                                    , 'application/x-www-form-urlencoded' \\\n",
    "                                    , max_parse_level = 1)\n",
    "\n",
    "# Header and column cleanup\n",
    "my_tracks.columns = my_tracks.columns.str.replace(f'{track_str}.', '', regex = False)\n",
    "my_tracks.rename(columns = {'id': track_id_str}, inplace = True)\n",
    "\n",
    "my_tracks[added_at_str] = pd.to_datetime(my_tracks[added_at_str])\n",
    "\n",
    "# Comment out the following line for personal uses\n",
    "# my_tracks[name_str] = my_tracks[name_str].apply(hash)\n",
    "\n",
    "display(my_tracks.head())\n",
    "print(my_tracks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd899b",
   "metadata": {},
   "source": [
    "Create the first CSV file for upload into other applications (see the README)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_write_df_to_data_out_csv(my_tracks, 'track_details_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207822d",
   "metadata": {},
   "source": [
    "In the track details, artist information is contained with a JSON field. Here, we unroll that JSON for each row into a DataFrame and add the corresponding \"*track_id*\" for the row. We then inspect the first few rows and the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9a1c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_str = 'artists'\n",
    "\n",
    "track_artists_df = convert_json_col_to_dataframe_with_key(my_tracks, track_id_str, artists_str)\n",
    "\n",
    "# Comment out the following line for personal uses\n",
    "# track_artists_df[name_str] = track_artists_df[name_str].apply(hash)\n",
    "\n",
    "display(track_artists_df.head())\n",
    "print(track_artists_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd7691",
   "metadata": {},
   "source": [
    "Create the next CSV file for upload into other applications (see the README)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11cd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_write_df_to_data_out_csv(track_artists_df, 'track_artists_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91216193",
   "metadata": {},
   "source": [
    "Next, we see how many liked tracks each artist has and do a quick plot using `matplotlib.pyplot` to visualize the top artists by liked track count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_str = 'id'\n",
    "count_track_id_str = f'count_{track_id_str}'\n",
    "\n",
    "# Use the DataFrame linking tracks to artists to get the number of tracks liked per artist.\n",
    "num_tracks_per_artist = track_artists_df.groupby([id_str, name_str])[track_id_str].count() \\\n",
    "                                        .to_frame().sort_values(track_id_str, ascending = False) \\\n",
    "                                        .reset_index().rename(columns = {track_id_str: count_track_id_str})\n",
    "\n",
    "hash_str = 'hash'\n",
    "num_tracks_per_artist[hash_str] = num_tracks_per_artist[name_str].apply(hash)\n",
    "\n",
    "num_tracks_per_artist.head(5).plot(kind = 'bar', x = hash_str, y = count_track_id_str, rot = 45)\n",
    "\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca63b0c",
   "metadata": {},
   "source": [
    "Let's see what else is in the data by looking at all artists who have more than 1 liked song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9ea55",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(num_tracks_per_artist[num_tracks_per_artist[count_track_id_str] >= 2][[name_str, count_track_id_str]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b0453",
   "metadata": {},
   "source": [
    "Remove unneeded columns from the data and display to ensure it looks as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98e6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_track_artists = num_tracks_per_artist[[id_str, name_str, count_track_id_str]]\n",
    "\n",
    "display(my_track_artists.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab159893",
   "metadata": {},
   "source": [
    "### Timeline of When Tracked Are Liked <a name=\"timeline\"></a>\n",
    "\n",
    "Next up in our analysis, we ultimately a flat file to see how liked artists' track count has grown over time. One limitation of this analysis is that we do not know when tracks are potentially *unliked.* To do so, a pipeline could be developed to poll this endpoint and create snapshots, but that is out of scope for this application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f26c1",
   "metadata": {},
   "source": [
    "In this block, we do some setup. We calculate all YYYY-MM formatted dates for which the user liked songs, filling in dates where no songs were liked along the way. Then, we create a template DataFrame for our analysis with *all* dates for *all* artists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable setup\n",
    "added_at_ym_str = added_at_str + '_ym'\n",
    "added_at_ymd_str = added_at_ym_str + 'd'\n",
    "\n",
    "# Pull in the added_at field for each track\n",
    "track_artists_df_with_added_at = pd.merge(track_artists_df, my_tracks[[track_id_str, added_at_str]] \\\n",
    "                                         , on = track_id_str)\n",
    "\n",
    "# Create fields for added_at formatted as YYYY-MM and YYYY-MM-DD\n",
    "track_artists_df_with_added_at[added_at_ym_str] = track_artists_df_with_added_at[added_at_str].dt.strftime('%Y-%m')\n",
    "track_artists_df_with_added_at[added_at_ymd_str] = track_artists_df_with_added_at[added_at_str].dt.date\n",
    "\n",
    "# Create a DataFrame of all YYYY-MM dates between the minimum added_at and maximum added_at\n",
    "added_at_date_range_pd_series = track_artists_df_with_added_at[added_at_ymd_str]\n",
    "\n",
    "added_at_date_range_df = pd.date_range(start = added_at_date_range_pd_series.min() \\\n",
    "                                       , end = added_at_date_range_pd_series.max()) \\\n",
    "                             .strftime('%Y-%m').drop_duplicates() \\\n",
    "                             .to_frame().reset_index(drop = True).rename(columns = {0: added_at_ym_str})\n",
    "\n",
    "# For each unique artist with tracks liked, create a DataFrame of all possible added_at YYYY-MM combinations\n",
    "# with that artist's information, and bind the rows of all DataFrames together\n",
    "unique_artists_df = track_artists_df_with_added_at[[id_str, name_str]].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "unique_artists_all_dates_list = list()\n",
    "    \n",
    "for i, df_row in unique_artists_df.iterrows():\n",
    "    tmp_unique_artist_base = unique_artists_df.iloc[[i]].reset_index(drop = True)\n",
    "    \n",
    "    tmp_unique_artist = pd.concat([added_at_date_range_df, tmp_unique_artist_base], axis = 1)\n",
    "    \n",
    "    for curr_col in tmp_unique_artist_base.columns:\n",
    "        tmp_unique_artist[curr_col].fillna(df_row[curr_col], inplace = True)\n",
    "    \n",
    "    unique_artists_all_dates_list.append(tmp_unique_artist)\n",
    "\n",
    "unique_artists_all_dates_df = pd.concat(unique_artists_all_dates_list).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069859c0",
   "metadata": {},
   "source": [
    "Here, we essentially fill-in the template we just created with liked songs per YYYY-MM per artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca3c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of liked songs per artist per YYYY-MM\n",
    "artist_month_indexes = [id_str, name_str, added_at_ym_str]\n",
    "\n",
    "num_tracks_per_artist_month = track_artists_df_with_added_at.groupby(artist_month_indexes)[track_id_str].count() \\\n",
    "                                        .to_frame().sort_values(added_at_ym_str) \\\n",
    "                                        .reset_index().rename(columns = {track_id_str: count_track_id_str})\n",
    "\n",
    "# Outer merge with all possible liked song dates, and fill NAs with 0.\n",
    "num_tracks_per_artist_month = pd.merge(num_tracks_per_artist_month \\\n",
    "                                       , unique_artists_all_dates_df \\\n",
    "                                       , on = artist_month_indexes \\\n",
    "                                       , how = 'outer' \\\n",
    "                                       , sort = True).fillna(0)\n",
    "\n",
    "display(num_tracks_per_artist_month.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21284edf",
   "metadata": {},
   "source": [
    "Now, it gets a little more interesting. We add a running sum to see growth over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column that calculates the running sum of liked songs by ascending date for each artist.\n",
    "# Data was sorted in the previous code block\n",
    "runningsum_str = 'runningsum'\n",
    "\n",
    "num_tracks_per_artist_month[runningsum_str] = num_tracks_per_artist_month.groupby([id_str])[count_track_id_str].cumsum()\n",
    "\n",
    "display(num_tracks_per_artist_month.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1de93c",
   "metadata": {},
   "source": [
    "For the grand finale, we pivot the data so the dates are our columns, using the running sum values for the pivot table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e5e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data\n",
    "num_tracks_per_artist_month_pivot = num_tracks_per_artist_month.pivot_table(values = runningsum_str \\\n",
    "                                                                           , index = [id_str, name_str] \\\n",
    "                                                                           , columns = added_at_ym_str \\\n",
    "                                                                           , fill_value = 0)\n",
    "\n",
    "display(num_tracks_per_artist_month_pivot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d715be",
   "metadata": {},
   "source": [
    "Create the final CSV file for upload into other applications (see the README)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca4a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the final file\n",
    "num_tracks_per_artist_month_pivot.reset_index(inplace = True)\n",
    "\n",
    "spotify_write_df_to_data_out_csv(num_tracks_per_artist_month_pivot, 'num_tracks_per_artist_month_pivot_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f648ad",
   "metadata": {},
   "source": [
    "### Additional API Exploration <a name=\"additionalapiexploration\"></a>\n",
    "\n",
    "In this section, we preview other available data from the Spotify API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95871af",
   "metadata": {},
   "source": [
    "#### Long Term Top Tracks <a name=\"longtermtoptracks\"></a>\n",
    "\n",
    "Identify what Spotify has determined to be the user's long term top tracks by hitting the `/me/top/tracks` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcea5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_long_term_top_tracks = spotify_get_all_results(access_token \\\n",
    "                                                 , f'{spotify_api_endpoint}me/top/tracks' \\\n",
    "                                                 , 'application/json' \\\n",
    "                                                 , query = {'time_range': 'long_term'})\n",
    "\n",
    "# Comment out the following line for personal uses\n",
    "my_long_term_top_tracks[name_str] = my_long_term_top_tracks[name_str].apply(hash)\n",
    "\n",
    "display(my_long_term_top_tracks.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880864b4",
   "metadata": {},
   "source": [
    "#### Followed Artists <a name=\"followedartists\"></a>\n",
    "\n",
    "In this section, we will download the user's followed artists in the Spotify application, and determine if any followed artists do not have any liked songs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fce548",
   "metadata": {},
   "source": [
    "First, let's call the `/me/following` endpoint to get the user's followed artists, and then display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f24da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_followed_artists = spotify_get_all_results(access_token \\\n",
    "                                              , f'{spotify_api_endpoint}me/following' \\\n",
    "                                              , 'application/json' \\\n",
    "                                              , query = {'type': 'artist'} \\\n",
    "                                              , base_obj = 'artists')\n",
    "\n",
    "# Comment out the following line for personal uses\n",
    "# my_followed_artists[name_str] = my_followed_artists[name_str].apply(hash)\n",
    "\n",
    "display(my_followed_artists.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd582ae1",
   "metadata": {},
   "source": [
    "Next, let's use an inner join to narrow down all of the user's liked track artists to just the artists the user follows. In other words, we obtain a DataFrame showing all of the artists that are both followed AND have liked songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f582eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_left = my_track_artists\n",
    "my_right = my_followed_artists\n",
    "\n",
    "my_followed_and_liked_artists_df = pd.merge(my_left \\\n",
    "                                            , my_right \\\n",
    "                                            , on = id_str \\\n",
    "                                            , how = 'inner' \\\n",
    "                                            , suffixes = ('', '_y'))[my_left.columns]\n",
    "\n",
    "display(my_followed_and_liked_artists_df.head())\n",
    "print(my_followed_and_liked_artists_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2988b45",
   "metadata": {},
   "source": [
    "Next, we use Python's Set type to create unique groups of artist IDs. We obtain all unique artist IDs with liked songs, all unique artist IDs who are being followed, and all unique artist IDs who are being followed and have liked songs.\n",
    "\n",
    "We can then use set operations to obtain the individual pieces of this Venn diagram and remove the overlaps. By doing so, we can determine if there are artists we **LIKE** but do not **FOLLOW**, or if there are artists we **FOLLOW** that we do not **LIKE.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0737ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sets\n",
    "my_track_artists_ids = set(my_left[id_str])\n",
    "my_followed_artists_ids = set(my_right[id_str])\n",
    "my_followed_and_liked_artists_ids = set(my_followed_and_liked_artists_df[id_str])\n",
    "\n",
    "# Use Set operations\n",
    "my_unfollowed_track_artists_ids = my_track_artists_ids - my_followed_and_liked_artists_ids\n",
    "my_followed_and_nonliked_artists_ids = my_followed_artists_ids - my_followed_and_liked_artists_ids\n",
    "\n",
    "# TODO handle when my_followed_and_nonliked_artists_ids is non-empty\n",
    "\n",
    "# Inspect data\n",
    "my_unfollowed_track_artists_df = my_left[my_left[id_str].isin(my_unfollowed_track_artists_ids)]\n",
    "\n",
    "my_top_unfollowed_artists_indices = my_unfollowed_track_artists_df[count_track_id_str] > 2\n",
    "\n",
    "my_top_unfollowed_artists_df = my_unfollowed_track_artists_df[my_top_unfollowed_artists_indices] \\\n",
    "                                    .reset_index(drop = True)\n",
    "\n",
    "my_top_unfollowed_artists_df[hash_str] = my_top_unfollowed_artists_df[name_str].apply(hash)\n",
    "\n",
    "display(my_top_unfollowed_artists_df[[hash_str, count_track_id_str]].head())\n",
    "print(my_top_unfollowed_artists_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
